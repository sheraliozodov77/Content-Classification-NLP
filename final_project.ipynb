{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cb06b4-f58d-46e0-8f71-b616bcc0512d",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "**Group HOMEWORK**. This final project can be collaborative. The maximum members of a group is 2. You can also work by yourself. Please respect the academic integrity. **Remember: if you get caught on cheating, you get F.**\n",
    "\n",
    "## A Introduction to the competition\n",
    "\n",
    "<img src=\"news-sexisme-EN.jpg\" alt=\"drawing\" width=\"380\"/>\n",
    "\n",
    "Sexism is a growing problem online. It can inflict harm on women who are targeted, make online spaces inaccessible and unwelcoming, and perpetuate social asymmetries and injustices. Automated tools are now widely deployed to find, and assess sexist content at scale but most only give classifications for generic, high-level categories, with no further explanation. Flagging what is sexist content and also explaining why it is sexist improves interpretability, trust and understanding of the decisions that automated tools use, empowering both users and moderators.\n",
    "\n",
    "This project is based on SemEval 2023 - Task 10 - Explainable Detection of Online Sexism (EDOS). [Here](https://codalab.lisn.upsaclay.fr/competitions/7124#learn_the_details-overview) you can find a detailed introduction to this task.\n",
    "\n",
    "You only need to complete **TASK A - Binary Sexism Detection: a two-class (or binary) classification where systems have to predict whether a post is sexist or not sexist**. To cut down training time, we only use a subset of the original dataset (5k out of 20k). The dataset can be found in the same folder. \n",
    "\n",
    "Different from our previous homework, this competition gives you great flexibility (and very few hints), you can determine: \n",
    "-  how to preprocess the input text (e.g., remove emoji, remove stopwords, text lemmatization and stemming, etc.);\n",
    "-  which method to use to encode text features (e.g., TF-IDF, N-grams, Word2vec, GloVe, Part-of-Speech (POS), etc.);\n",
    "-  which model to use.\n",
    "\n",
    "## Requirements\n",
    "-  **Input**: the text for each instance.\n",
    "-  **Output**: the binary label for each instance.\n",
    "-  **Feature engineering**: use at least 2 different methods to extract features and encode text into numerical values.\n",
    "-  **Model selection**: implement with at least 3 different models and compare their performance.\n",
    "-  **Evaluation**: create a dataframe with rows indicating feature+model and columns indicating Precision, Accuracy and F1-score (using weighted average). Your results should have at least 6 rows (2 feature engineering methods x 3 models). Report best performance with (1) your feature engineering method, and (2) the model you choose. \n",
    "- **Format**: add explainations for each step (you can add markdown cells). At the end of the report, write a summary and answer the following questions: \n",
    "    - What preprocessing steps do you follow?\n",
    "    - How do you select the features from the inputs? \n",
    "    - Which model you use and what is the structure of your model?\n",
    "    - How do you train your model?\n",
    "    - What is the performance of your best model?\n",
    "    - What other models or feature engineering methods would you like to implement in the future?\n",
    "- **Two Rules**, violations will result in 0 points in the grade: \n",
    "    - Not allowed to use test set in the training: You CANNOT use any of the instances from test set in the training process. \n",
    "    - Not allowed to use any generative AI (e.g., ChatGPT). \n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The performance should be only evaluated on the test set (a total of 1086 instances). Please split original dataset into train set and test set. The test set should NEVER be used in the training process. The evaluation metric is a combination of precision, recall, and f1-score (use `classification_report` in sklearn). \n",
    "\n",
    "The total points are 10.0. Each team will compete with other teams in the class on their best performance. Points will be deducted if not following the requirements above. \n",
    "\n",
    "If ALL the requirements are met:\n",
    "- Top 25\\% teams: 10.0 points.\n",
    "- Top 25\\% - 50\\% teams: 8.5 points.\n",
    "- Top 50\\% - 75\\% teams: 7.0 points.\n",
    "- Top 75\\% - 100\\% teams: 6.0 points.\n",
    "\n",
    "If your best performance is above 0.80 (weighted F1-score) and meets all the requirements, you will also get full points (10.0 points). \n",
    "\n",
    "⭐ Bonus points will be awarded to top 5 teams (ranked by weighted F1-score):\n",
    "- Top 1 team: 3pt adding to final grade\n",
    "- Top 2 team: 2pt adding to final grade\n",
    "- Top 3-5 teams: 1pt adding to final grade\n",
    "\n",
    "## Submission\n",
    "Similar as homework, submit both a PDF and .ipynb version of the report. \n",
    "\n",
    "The report should include: \n",
    "- (a)code AND outputs\n",
    "- (b)explainations for each step \n",
    "- (c)individual experimental results AND combine them in a table \n",
    "- (d)summary \n",
    "\n",
    "The due date is **May 2, Thursday by 11:59pm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sheraliozodov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sheraliozodov/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sheraliozodov/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sheraliozodov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download(['punkt', 'averaged_perceptron_tagger', 'wordnet', 'stopwords'])\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape: (4193, 4)\n",
      "Testing Set Shape: (1086, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('edos_labelled_data.csv')\n",
    "train_data = data[data['split'] == 'train'].copy()\n",
    "test_data = data[data['split'] == 'test'].copy()\n",
    "\n",
    "# Display dataset structure\n",
    "print(\"Training Set Shape:\", train_data.shape)\n",
    "print(\"Testing Set Shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The raw data requires cleaning and preprocessing to be suitable for modeling. We will remove URLs, user tags, non-alphanumeric characters, and numbers from the texts. Additionally, the texts will be tokenized and lemmatized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Preprocessing\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\[USER\\]', '', text) # Remove user tags\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    return text.strip().lower()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Converts treebank tags to wordnet tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "# Apply preprocessing\n",
    "train_data['clean_text'] = train_data['text'].apply(preprocess)\n",
    "test_data['clean_text'] = test_data['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Lemmatization Function\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatizes text using POS tags to ensure accurate lemmatization.\"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    words_pos = pos_tag(words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for word, pos in words_pos:\n",
    "        if word not in stop_words:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "            lemmatized_words.append(lemmatized_word)\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Apply tokenization and lemmatization\n",
    "train_data['lemmatized_text'] = train_data['clean_text'].apply(lemmatize_text)\n",
    "test_data['lemmatized_text'] = test_data['clean_text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "To represent text data numerically, we utilize two methods: TF-IDF Vectorization and Count Vectorization. Each method transforms the text into a vector that machine learning models can process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['lemmatized_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data['lemmatized_text'])\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=2000, ngram_range=(1, 2))\n",
    "X_train_count = count_vectorizer.fit_transform(train_data['lemmatized_text'])\n",
    "X_test_count = count_vectorizer.transform(test_data['lemmatized_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the Labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_data['label'])\n",
    "y_test = label_encoder.transform(test_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "We will train and evaluate three models: Logistic Regression, Random Forest, and Support Vector Machine (SVM), using both feature sets (TF-IDF and Count Vectors) to determine which combination yields the best performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Results for {model_name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Accuracy: {:.2f}\\n\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance:\n",
      "Results for Logistic Regression TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88       789\n",
      "           1       0.72      0.50      0.59       297\n",
      "\n",
      "    accuracy                           0.81      1086\n",
      "   macro avg       0.78      0.71      0.73      1086\n",
      "weighted avg       0.80      0.81      0.80      1086\n",
      "\n",
      "Confusion Matrix:\n",
      " [[731  58]\n",
      " [148 149]]\n",
      "Accuracy: 0.81\n",
      "\n",
      "Results for Logistic Regression Count Vectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87       789\n",
      "           1       0.69      0.48      0.57       297\n",
      "\n",
      "    accuracy                           0.80      1086\n",
      "   macro avg       0.76      0.70      0.72      1086\n",
      "weighted avg       0.79      0.80      0.79      1086\n",
      "\n",
      "Confusion Matrix:\n",
      " [[724  65]\n",
      " [154 143]]\n",
      "Accuracy: 0.80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "print(\"Logistic Regression Performance:\")\n",
    "lr_params = {'C': np.logspace(-3, 3, 10),\n",
    "             'penalty': ['l1', 'l2'], \n",
    "             'solver': ['liblinear', 'saga']}\n",
    "\n",
    "grid_lr_tfidf = GridSearchCV(LogisticRegression(), lr_params, cv=5, scoring='f1_weighted')\n",
    "grid_lr_tfidf.fit(X_train_tfidf, y_train)\n",
    "evaluate_model(grid_lr_tfidf.best_estimator_, X_test_tfidf, y_test, \"Logistic Regression TF-IDF\")\n",
    "\n",
    "grid_lr_count = GridSearchCV(LogisticRegression(), lr_params, cv=5, scoring='f1_weighted')\n",
    "grid_lr_count.fit(X_train_count, y_train)\n",
    "evaluate_model(grid_lr_count.best_estimator_, X_test_count, y_test, \"Logistic Regression Count Vectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance:\n",
      "Results for Random Forest TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88       789\n",
      "           1       0.78      0.45      0.58       297\n",
      "\n",
      "    accuracy                           0.82      1086\n",
      "   macro avg       0.80      0.70      0.73      1086\n",
      "weighted avg       0.81      0.82      0.80      1086\n",
      "\n",
      "Confusion Matrix:\n",
      " [[752  37]\n",
      " [162 135]]\n",
      "Accuracy: 0.82\n",
      "\n",
      "Results for Random Forest Count Vectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.87       789\n",
      "           1       0.71      0.49      0.58       297\n",
      "\n",
      "    accuracy                           0.81      1086\n",
      "   macro avg       0.77      0.71      0.73      1086\n",
      "weighted avg       0.80      0.81      0.80      1086\n",
      "\n",
      "Confusion Matrix:\n",
      " [[730  59]\n",
      " [150 147]]\n",
      "Accuracy: 0.81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "print(\"Random Forest Performance:\")\n",
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30, 40]\n",
    "}\n",
    "\n",
    "grid_rf_tfidf = GridSearchCV(RandomForestClassifier(), rf_params, cv=5, scoring='f1_weighted')\n",
    "grid_rf_tfidf.fit(X_train_tfidf, y_train)\n",
    "evaluate_model(grid_rf_tfidf.best_estimator_, X_test_tfidf, y_test, \"Random Forest TF-IDF\")\n",
    "\n",
    "grid_rf_count = GridSearchCV(RandomForestClassifier(), rf_params, cv=5, scoring='f1_weighted')\n",
    "grid_rf_count.fit(X_train_count, y_train)\n",
    "evaluate_model(grid_rf_count.best_estimator_, X_test_count, y_test, \"Random Forest Count Vectorizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Performance:\n",
      "Results for Optimized SVM TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.84      0.85       789\n",
      "           1       0.59      0.62      0.61       297\n",
      "\n",
      "    accuracy                           0.78      1086\n",
      "   macro avg       0.72      0.73      0.73      1086\n",
      "weighted avg       0.78      0.78      0.78      1086\n",
      "\n",
      "Confusion Matrix:\n",
      " [[662 127]\n",
      " [113 184]]\n",
      "Accuracy: 0.78\n",
      "\n",
      "Results for SVM Count Vectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85       789\n",
      "           1       0.60      0.63      0.61       297\n",
      "\n",
      "    accuracy                           0.78      1086\n",
      "   macro avg       0.73      0.73      0.73      1086\n",
      "weighted avg       0.79      0.78      0.78      1086\n",
      "\n",
      "Confusion Matrix:\n",
      " [[662 127]\n",
      " [110 187]]\n",
      "Accuracy: 0.78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine\n",
    "print(\"SVM Performance:\")\n",
    "\n",
    "svm_params = {\n",
    "    'C': np.logspace(-2, 2, 8),\n",
    "    'gamma': np.logspace(-2, 2, 8),  \n",
    "    'kernel': ['rbf'],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "grid_svm_tfidf = GridSearchCV(SVC(), svm_params, cv=5, scoring='f1_weighted')\n",
    "grid_svm_tfidf.fit(X_train_tfidf, y_train)\n",
    "evaluate_model(grid_svm_tfidf.best_estimator_, X_test_tfidf, y_test, \"Optimized SVM TF-IDF\")\n",
    "\n",
    "grid_svm_count = GridSearchCV(SVC(), svm_params, cv=3, scoring='f1_weighted')\n",
    "grid_svm_count.fit(X_train_count, y_train)\n",
    "evaluate_model(grid_svm_count.best_estimator_, X_test_count, y_test, \"SVM Count Vectorizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC Performance:\n",
      "Results for LinearSVC TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88       789\n",
      "           1       0.77      0.45      0.57       297\n",
      "\n",
      "    accuracy                           0.81      1086\n",
      "   macro avg       0.80      0.70      0.73      1086\n",
      "weighted avg       0.81      0.81      0.80      1086\n",
      "\n",
      "Confusion Matrix:\n",
      " [[750  39]\n",
      " [163 134]]\n",
      "Accuracy: 0.81\n",
      "\n",
      "Results for LinearSVC Count Vectorizer:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.87       789\n",
      "           1       0.66      0.53      0.59       297\n",
      "\n",
      "    accuracy                           0.80      1086\n",
      "   macro avg       0.75      0.71      0.73      1086\n",
      "weighted avg       0.79      0.80      0.79      1086\n",
      "\n",
      "Confusion Matrix:\n",
      " [[710  79]\n",
      " [141 156]]\n",
      "Accuracy: 0.80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LinearSVC\n",
    "print(\"LinearSVC Performance:\")\n",
    "linear_svm_params = {\n",
    "    'C': np.logspace(-5, 2, 8),\n",
    "    'loss': ['hinge', 'squared_hinge']\n",
    "}\n",
    "\n",
    "grid_linear_svm_tfidf = GridSearchCV(LinearSVC(), linear_svm_params, cv=5, scoring='f1_weighted')\n",
    "grid_linear_svm_tfidf.fit(X_train_tfidf, y_train)\n",
    "evaluate_model(grid_linear_svm_tfidf.best_estimator_, X_test_tfidf, y_test, \"LinearSVC TF-IDF\")\n",
    "\n",
    "grid_linear_svm_count = GridSearchCV(LinearSVC(), linear_svm_params, cv=5, scoring='f1_weighted')\n",
    "grid_linear_svm_count.fit(X_train_count, y_train)\n",
    "evaluate_model(grid_linear_svm_count.best_estimator_, X_test_count, y_test, \"LinearSVC Count Vectorizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental results\n",
    "Please organize your results similar as the following table (you can choose other ways to display your table)\n",
    "\n",
    "<img src=\"model_performance.jpeg\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7827b394-47a9-4ccc-8ee3-9a9059064cff",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### 1. What preprocessing steps do you follow?\n",
    "\n",
    "**Answer**: Preprocessing involved several steps to clean and standardize the input text data:\n",
    "- **URLs and User Tags Removal**: Stripping out hyperlinks and user mentions, which are irrelevant to text classification.\n",
    "- **Punctuation and Numbers Removal**: Deleting punctuation and numerical figures to focus on textual content.\n",
    "- **Lowercasing All Text**: Standardizing all text to lower case to avoid duplication of tokens based on case differences.\n",
    "- **Tokenization**: Breaking down text into individual words or tokens.\n",
    "- **Lemmatization with POS Tagging**: Converting words to their base form using part-of-speech tagging to accurately derive the lemma.\n",
    "- **Stop Words Removal**: Eliminating commonly used words (such as “the”, “a”, “in”) that do not contribute significantly to the meaning of the text.\n",
    "\n",
    "### 2. How do you select the features from the inputs?\n",
    "\n",
    "**Answer**: Features were selected using two primary text representation techniques:\n",
    "- **TF-IDF Vectorization**: This method weighs words based on their frequency and relevance across documents, helping to elevate words that are unique to a document’s context.\n",
    "- **Count Vectorization**: This approach counts the occurrences of each word in the text to form feature vectors.\n",
    "\n",
    "### 3. Which model you use and what is the structure of your model?\n",
    "\n",
    "**Answer**: The models tested included:\n",
    "- **Random Forest**: An ensemble model using multiple decision trees to improve classification accuracy and control over-fitting.\n",
    "- **Support Vector Machine (SVM)**: Employs kernels to handle nonlinear data separation effectively.\n",
    "- **LinearSVC**: A linear type of SVM that is faster on large datasets and effective with high dimensional spaces.\n",
    "Each model was configured with hyperparameters optimized through cross-validation using `GridSearchCV`.\n",
    "\n",
    "### 4. How do you train your model?\n",
    "\n",
    "**Answer**: Training involved the following steps:\n",
    "- **Splitting Data**: Dividing data into training and testing sets to ensure the model is validated on unseen data.\n",
    "- **Applying Vectorizers**: Transforming text data into numerical formats using TF-IDF and Count Vectorizers.\n",
    "- **Grid Search CV**: Utilizing GridSearchCV to search through predefined hyperparameter spaces to find the optimal settings for each model.\n",
    "- **Model Fitting**: Training each model on the vectorized training data and tuning them based on the validation results from the cross-validation process.\n",
    "\n",
    "### 5. What is the performance of your best model?\n",
    "\n",
    "**Answer**: The **Random Forest with TF-IDF Vectorizer** has shown to be the best performing model according to the metrics evaluated on the test set:\n",
    "- **Accuracy**: 82%\n",
    "- **Precision**: 81% (weighted average)\n",
    "- **Recall**: 82% (weighted average)\n",
    "- **F1-Score**: 80% (weighted average)\n",
    "- **Confusion Matrix**: True Negative = 752, False Positive = 37, False Negative = 162, True Positive = 135\n",
    "\n",
    "### 6. What other models or feature engineering methods would you like to implement in the future?\n",
    "\n",
    "**Answer**: Future directions could include:\n",
    "- **Enhanced Feature Engineering**: Incorporating Word2Vec or GloVe to potentially improve model performance.\n",
    "- **Hyperparameter Optimization**: Further refining models through more sophisticated hyperparameter optimization techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
